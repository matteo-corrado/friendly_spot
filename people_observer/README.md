# people_observer

Real-time person detection and tracking with PTZ following. Uses GPU-accelerated YOLO on Spot's surround fisheye cameras to detect people, maps detections to robot-frame bearings using SDK intrinsics and transforms, and aims the Spot CAM PTZ at tracked targets.

## Architecture Overview

### System Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER EXECUTION                                             â”‚
â”‚  python -m people_observer.app <ROBOT_IP> [--mode transform|bearing]                    â”‚
â”‚                                [--visualize] [--save-images DIR]                        â”‚
â”‚                                [--dry-run] [--once] [--exit-on-detection]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              app.py (Entry Point)                           â”‚
â”‚  - Parse CLI arguments                                                      â”‚
â”‚  - Create RuntimeConfig from config.py                                      â”‚
â”‚  - Connect to robot (io_robot.connect)                                      â”‚
â”‚  - Initialize clients (ImageClient, PtzClient, CompositorClient)            â”‚
â”‚  - Configure stream settings                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           config.py (Configuration)                                     â”‚
â”‚  Constants:                                   Dataclasses:                              â”‚
â”‚  - SURROUND_SOURCES (5 cameras)               - RuntimeConfig                           â”‚
â”‚  - DEFAULT_YOLO_MODEL (yolov8n.pt)            - YOLOConfig                              â”‚
â”‚  - YOLO_DEVICE (cuda/cpu)                     - PTZConfig                               â”‚
â”‚  - MIN_CONFIDENCE (0.30)                      - ConnectionConfig                        â”‚
â”‚  - LOOP_HZ (7)                                                                          â”‚
â”‚  - PTZ_NAME (mech), COMPOSITOR_SCREEN         observer_mode: str (transform/bearing)    â”‚
â”‚  - DEFAULT_ZOOM (1.0)                         Intrinsics fetched at runtime from SDK    â”‚
â”‚  - TRANSFORM_MODE (transform - default)                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          tracker.run_loop() (Main Loop)                      â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7 Hz Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
â”‚  â”‚                                                                             â”‚
â”‚  â”‚  1. Fetch Frames (cameras.py)                                               â”‚
â”‚  â”‚     â”œâ”€> ImageClient.get_image_from_sources()                                â”‚
â”‚  â”‚     â”œâ”€> Decode JPEG/RAW to BGR numpy arrays                                 â”‚
â”‚  â”‚     â””â”€> Return: dict[camera_name -> image], ImageResponses                  â”‚        â”‚
â”‚  â”‚         (ImageResponse contains frame_tree for transforms)                  â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  2. Run Detection (detection.py)                                            â”‚        â”‚
â”‚  â”‚     â”œâ”€> YoloDetector.predict_batch()                                        â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> GPU inference (CUDA) with FP16 if available                     â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> CPU fallback if GPU unavailable                                 â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> Filter: person class only (class_id=0)                          â”‚        â”‚
â”‚  â”‚     â”‚   â””â”€> Apply confidence threshold (0.30)                               â”‚        â”‚
â”‚  â”‚     â””â”€> Return: list[list[Detection]] per camera                            â”‚        â”‚
â”‚  â”‚         Detection: {source, bbox_xywh, conf}                                â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  3. Select Target Person (tracker.py)                                       â”‚        â”‚
â”‚  â”‚     â”œâ”€> Rank by depth if available (estimate_detection_depth_m)             â”‚        â”‚
â”‚  â”‚     â”œâ”€> Fallback: rank by largest bbox area (pick_largest)                  â”‚        â”‚
â”‚  â”‚     â”œâ”€> Filter by MIN_AREA_PX (600)                                         â”‚        â”‚
â”‚  â”‚     â””â”€> Return: best (camera_name, Detection, ImageResponse)                â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  4. Compute PTZ Angles (geometry.py)                                        â”‚        â”‚
â”‚  â”‚     â”‚                                                                       â”‚        â”‚
â”‚  â”‚     â”œâ”€> Mode: "transform" (DEFAULT - accurate)                              â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> Get camera intrinsics from ImageSource                          â”‚        â”‚
â”‚  â”‚     â”‚   â”‚   â””â”€> Kannala-Brandt (k1-k4) for fisheye                          â”‚        â”‚
â”‚  â”‚     â”‚   â”‚   â””â”€> Pinhole (fx, fy, cx, cy) for PTZ/hand                       â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> pixel_to_camera_ray() using cv2.fisheye.undistortPoints         â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> Transform ray: camera frame â†’ body frame (SE3Pose)              â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> Body bearing = atan2(ray.y, ray.x) [-180, 180]                  â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> Body tilt = atan2(ray.z, hypot(x,y))                            â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> Convert body â†’ PTZ coordinates                                  â”‚        â”‚
â”‚  â”‚     â”‚   â”‚   â””â”€> PTZ pan = -bearing (flip left/right)                        â”‚        â”‚
â”‚  â”‚     â”‚   â”‚   â””â”€> Normalize pan to [0, 360]                                   â”‚        â”‚
â”‚  â”‚     â”‚   â”‚   â””â”€> PTZ tilt = body tilt                                        â”‚        â”‚
â”‚  â”‚     â”‚   â””â”€> Return: (pan_deg, tilt_deg)                                     â”‚        â”‚
â”‚  â”‚     â”‚                                                                       â”‚        â”‚
â”‚  â”‚     â””â”€> Mode: "bearing" (fallback, no intrinsics)                           â”‚        â”‚
â”‚  â”‚         â”œâ”€> pixel_to_ptz_angles_simple()                                    â”‚        â”‚
â”‚  â”‚         â”œâ”€> Calculate HFOV from intrinsics or use fallback (133Â°)           â”‚        â”‚
â”‚  â”‚         â”œâ”€> pixel_offset = HFOV Ã— (pixel_x/width - 0.5)                     â”‚        â”‚
â”‚  â”‚         â”œâ”€> camera_yaw from CAM_YAW_DEG config                              â”‚        â”‚
â”‚  â”‚         â””â”€> pan_deg = camera_yaw + pixel_offset                             â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  5. Command PTZ (ptz_control.py)                                            â”‚        â”‚
â”‚  â”‚     â”œâ”€> Query current PTZ position (get_ptz_position)                       â”‚        â”‚
â”‚  â”‚     â”œâ”€> Validate angles: pan [0,360], tilt [-30,100], zoom [1.0,30.0]       â”‚        â”‚
â”‚  â”‚     â”œâ”€> Clamp to valid ranges if needed                                     â”‚        â”‚
â”‚  â”‚     â”œâ”€> set_ptz(pan_deg, tilt_deg, zoom=1.0)                                â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> PtzDescription(name="mech")                                     â”‚        â”‚
â”‚  â”‚     â”‚   â”œâ”€> PtzClient.set_ptz_position(desc, pan, tilt, zoom)               â”‚        â”‚
â”‚  â”‚     â”‚   â””â”€> Log success/failure with detailed error info                    â”‚        â”‚
â”‚  â”‚     â””â”€> If dry_run: log only, skip command                                  â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  6. Visualization (optional, visualization.py)                              â”‚        â”‚
â”‚  â”‚     â”œâ”€> draw_detections() on each camera frame                              â”‚        â”‚
â”‚  â”‚     â”œâ”€> create_grid_layout() 3x2 grid for 5 cameras                         â”‚        â”‚
â”‚  â”‚     â”œâ”€> show_detections_grid() OpenCV window                                â”‚        â”‚
â”‚  â”‚     â””â”€> Handle keyboard: 'q' quit, ESC quit                                 â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  7. Save Images (optional, visualization.py)                                â”‚        â”‚
â”‚  â”‚     â”œâ”€> save_annotated_frames() if --save-images DIR                        â”‚        â”‚
â”‚  â”‚     â”œâ”€> Timestamp-based unique filenames (YYYYMMDD_HHMMSS_mmm)              â”‚        â”‚
â”‚  â”‚     â”œâ”€> Format: {timestamp}_iter{iteration:04d}_{camera}.jpg                â”‚        â”‚
â”‚  â”‚     â””â”€> Validate images before saving (check not None/empty)                â”‚        â”‚
â”‚  â”‚                                                                             â”‚        â”‚
â”‚  â”‚  8. Loop Pacing                                                             â”‚        
â”‚  â”‚     â””â”€> sleep() to maintain LOOP_HZ (7 Hz = ~143ms)                         â”‚        
â”‚  â”‚                                                               â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                                â”‚
â”‚  Exit Conditions:                                                              â”‚
â”‚  - cfg.once = True: exit after 1 iteration                                     â”‚
â”‚  - cfg.exit_on_detection = True: exit after successful PTZ command             â”‚
â”‚  - KeyboardInterrupt (Ctrl+C)                                                  â”‚
â”‚  - Visualization window: 'q' or ESC pressed                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

```

### Module Interaction Diagram

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     app.py       â”‚  Entry point, CLI parsing
                         â”‚   (main entry)   â”‚  Initializes clients & config
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                   â”‚                    â”‚
              â–¼                   â–¼                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ config.py  â”‚      â”‚io_robot.py â”‚      â”‚ tracker.py â”‚
       â”‚            â”‚      â”‚            â”‚      â”‚            â”‚
       â”‚ Runtime    â”‚      â”‚ - connect()â”‚      â”‚ - run_loop â”‚
       â”‚ Config     â”‚      â”‚ - ensure   â”‚      â”‚ - main     â”‚
       â”‚ Constants  â”‚      â”‚   _clients â”‚      â”‚   detectionâ”‚
       â”‚ Dataclassesâ”‚      â”‚ - configureâ”‚      â”‚   loop     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   _stream  â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚
                                  â”‚                   â”‚
                                  â”‚ ImageClient       â”‚
                                  â”‚ PtzClient         â”‚
                                  â”‚ CompositorClient  â”‚
                                  â”‚                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                                 â”‚
                    â–¼                                                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     cameras.py       â”‚                         â”‚  detection.py    â”‚
         â”‚                      â”‚                         â”‚                  â”‚
         â”‚ - fetch_image_sourcesâ”‚  ImageResponse          â”‚ - YoloDetector   â”‚
         â”‚   (intrinsics cache) â”‚  (with frame_tree)      â”‚ - GPU/CPU auto   â”‚
         â”‚ - get_camera         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ - predict_batch()â”‚
         â”‚   _intrinsics()      â”‚                         â”‚ - FP16 precision â”‚
         â”‚ - pixel_to_camera_rayâ”‚  BGR numpy arrays       â”‚ - person filter  â”‚
         â”‚   (undistort)        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                  â”‚
         â”‚ - calculate_hfov()   â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ - get_frames()       â”‚                                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
                    â”‚                                              â”‚
                    â”‚ Intrinsics:                                  â”‚
                    â”‚ - Kannala-Brandt (k1-k4)                     â”‚
                    â”‚ - Pinhole (fx, fy, cx, cy)                   â”‚
                    â”‚                                              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ frames + detections + intrinsics
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    geometry.py       â”‚
                       â”‚                      â”‚
                       â”‚ Transform Mode:      â”‚
                       â”‚ - pixel_to_ptz       â”‚
                       â”‚   _angles_transform()â”‚
                       â”‚   â€¢ cv2.fisheye      â”‚
                       â”‚     undistortPoints  â”‚
                       â”‚   â€¢ SDK frame_helpersâ”‚
                       â”‚     get_a_tform_b()  â”‚
                       â”‚   â€¢ bodyâ†’PTZ coords  â”‚
                       â”‚                      â”‚
                       â”‚ Bearing Mode:        â”‚
                       â”‚ - pixel_to_ptz       â”‚
                       â”‚   _angles_simple()   â”‚
                       â”‚   â€¢ HFOV projection  â”‚
                       â”‚   â€¢ camera yaw       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ (pan_deg, tilt_deg)
                                  â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   ptz_control.py     â”‚
                       â”‚                      â”‚
                       â”‚ - get_ptz_position() â”‚
                       â”‚   (query current)    â”‚
                       â”‚ - Validate ranges:   â”‚
                       â”‚   pan [0-360]        â”‚
                       â”‚   tilt [-30,100]     â”‚
                       â”‚   zoom [1.0-30.0]    â”‚
                       â”‚ - apply_deadband()   â”‚
                       â”‚ - clamp_step()       â”‚
                       â”‚ - set_ptz()          â”‚
                       â”‚   â€¢ Error handling   â”‚
                       â”‚   â€¢ Success logging  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ PtzClient API
                                  â–¼
                            Spot CAM PTZ
                             (hardware)
                          360Â° pan, Â±100Â° tilt

              Optional Visualization & Logging:
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  visualization.py    â”‚
                       â”‚                      â”‚
                       â”‚ - draw_detections()  â”‚
                       â”‚ - create_grid_layout â”‚
                       â”‚   (3x2 grid)         â”‚
                       â”‚ - show_detections    â”‚
                       â”‚   _grid()            â”‚
                       â”‚ - save_annotated     â”‚
                       â”‚   _frames()          â”‚
                       â”‚   â€¢ Timestamp naming â”‚
                       â”‚   â€¢ Never overwrites â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                           â–¼
            OpenCV Window                 Disk Storage
         (5 cameras, grid view)      (timestamped JPEGs)
           Press 'q' to quit          ./images/*.jpg
```

### Data Flow Details

**Frame Acquisition** (cameras.py):
```
ImageClient.get_image_from_sources(sources)
    â””â”€> List[ImageResponse]
        â””â”€> decode_image(response)
            â””â”€> np.ndarray (BGR, HxWx3)
```

**Detection** (detection.py):
```
YoloDetector.predict_batch(bgr_list)
    â””â”€> YOLO.predict(device=cuda/cpu, conf=0.30, classes=[0])
        â””â”€> Filter: person class only
            â””â”€> List[Detection(source, bbox_xywh, conf)]
```

**PTZ Angle Calculation** (geometry.py):
```
Transform Mode (DEFAULT - accurate with distortion correction):
  pixel (u,v)
    â””â”€> cameras.get_camera_intrinsics() [Kannala-Brandt k1-k4 or pinhole]
    â””â”€> cameras.pixel_to_camera_ray() [cv2.fisheye.undistortPoints()]
    â””â”€> frame_helpers.get_a_tform_b(frame_tree, body, camera) [SE3Pose]
    â””â”€> Transform ray: camera frame â†’ body frame [Vec3 multiply]
    â””â”€> bearing_rad = atan2(ray_body.y, ray_body.x) [-Ï€, Ï€]
    â””â”€> tilt_rad = atan2(ray_body.z, hypot(x,y))
    â””â”€> Convert body â†’ PTZ coordinates:
        â€¢ PTZ pan = -bearing (negate to flip left/right)
        â€¢ Normalize pan to [0, 360]
        â€¢ PTZ tilt = body tilt

Bearing Mode (fallback when intrinsics unavailable):
  pixel (u,v)
    â””â”€> pixel_offset = HFOV Ã— (pixel_x / width - 0.5)
    â””â”€> camera_yaw from CAM_YAW_DEG config
    â””â”€> pan_deg = camera_yaw + pixel_offset
    â””â”€> tilt_deg = DEFAULT_TILT_DEG (constant)
```

**PTZ Command** (ptz_control.py):
```
target_pan/tilt -> deadband filter -> step limiter
                -> degrees to radians
                -> PtzClient.set_ptz_position()
```

**Visualization** (visualization.py):
```
Per Frame (each camera):
  draw_detections(image, detections, camera_name)
    â”œâ”€> For each Detection:
    â”‚   â”œâ”€> Draw green bounding box (x, y, w, h)
    â”‚   â”œâ”€> Draw confidence label: "Person 0.87"
    â”‚   â””â”€> Black semi-transparent background for readability
    â”œâ”€> Camera name in top-left corner
    â””â”€> Detection count in top-right corner

Grid Layout:
  create_grid_layout(annotated_images, cols=3, target_width=1920)
    â”œâ”€> Resize each image to cell size (640x480)
    â”œâ”€> Arrange in 3-column grid (5 cameras = 2 rows)
    â””â”€> Fill remaining cells with black

Display:
  show_detections_grid(frames_dict, detections_dict)
    â”œâ”€> Annotate all frames
    â”œâ”€> Create grid layout
    â”œâ”€> Add stats panel: "Total detections: 2 across 5 cameras | Press 'q' to quit"
    â”œâ”€> cv2.imshow() - non-blocking (1ms wait)
    â””â”€> Return key code ('q' or ESC = quit)
```

### Visualization Module Details

The visualization system provides real-time feedback during detection and tracking operations.

#### Layout Structure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FrontLeft     â”‚   FrontRight    â”‚    Left         â”‚
â”‚   640x480       â”‚   640x480       â”‚   640x480       â”‚
â”‚  [Person 0.89]  â”‚  [Person 0.76]  â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Right         â”‚     Back        â”‚   (empty)       â”‚
â”‚   640x480       â”‚   640x480       â”‚   640x480       â”‚
â”‚                 â”‚  [Person 0.82]  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Total detections: 3 across 5 cameras
              Press 'q' to quit
```

#### Color Coding
- **Green boxes** (0, 255, 0): Person detections above confidence threshold
- **White text** (255, 255, 255): Confidence scores and labels
- **Black backgrounds** (0, 0, 0): Semi-transparent for text readability

#### Usage Modes

**1. Live Tracking with Visualization**
```powershell
python -m people_observer.app --hostname <IP> --visualize
```
- Updates at 7 Hz (synchronized with detection loop)
- Non-blocking display (1ms cv2.waitKey)
- Press 'q' or ESC to gracefully exit

**2. Dry-Run Debug Mode**
```powershell
python -m people_observer.app --hostname <IP> --dry-run --visualize
```
- See detections without PTZ commands
- Verify camera alignment and detection quality
- Check confidence thresholds visually

**3. Single Frame Capture**
```powershell
python -m people_observer.app --hostname <IP> --once --visualize
```
- Process one detection cycle
- View results, then exit
- Useful for testing camera positioning

#### Performance Characteristics
- **Overhead**: ~10-20ms per frame for annotation + display
- **Resolution**: 1920x1040 total (3x640 + stats panel)
- **Memory**: ~12MB for grid (5 cameras Ã— 640Ã—480Ã—3 bytes)
- **CPU Usage**: Minimal (OpenCV hardware-accelerated when available)

#### Integration with Tracker
```python
# In tracker.py main loop
if cfg.visualize:
    key = visualization.show_detections_grid(frames, detections_by_camera)
    if key == ord('q') or key == 27:  # 'q' or ESC
        logger.info("User requested quit via visualization")
        break
```

#### Debugging Features

**Camera Label Annotations:**
- Camera names cleaned: "frontleft_fisheye_image" â†’ "FRONTLEFT"
- Detection counts per camera
- Overall statistics across all cameras

**Bounding Box Information:**
- Position: Label placed above box (or below if top clipped)
- Format: "Person 0.87" (confidence to 2 decimal places)
- Clamping: Boxes constrained to image boundaries

**Save Frames (Optional):**
```python
visualization.save_annotated_frames(
    frames_dict,
    detections_dict,
    output_dir="./debug_frames",
    iteration=42
)
# Saves: debug_frames/iter0042_frontleft_fisheye_image.jpg (Ã—5)
```

#### Window Management
- **Window Name**: "People Observer - Detections"
- **Resize**: Auto-scaled to fit 1920px width
- **Position**: OS default (can be moved by user)
- **Focus**: Requires focus for keyboard input
- **Close**: Window closed automatically on exit

#### Constants (All Configurable in visualization.py)
```python
GRID_COLS = 3                    # Columns in grid
DEFAULT_TARGET_WIDTH = 1920      # Total grid width
GRID_ASPECT_RATIO = 3.0 / 4.0   # 4:3 for fisheye
THICKNESS = 2                    # Box line thickness
FONT_SCALE = 0.5                 # Label text size
CAMERA_LABEL_PADDING = 10        # Corner label spacing
CONFIDENCE_LABEL_PADDING = 4     # Box label spacing
STATS_PANEL_HEIGHT = 40          # Bottom panel height
WAIT_KEY_MS = 1                  # Non-blocking key check
```

## What's here
- `app.py` - Main entry point: orchestrates camera capture, detection, tracking, and PTZ aiming loop.
- `cameras.py` - Camera source management via `ImageClient`; handles multiple surround fisheye sources.
- `detection.py` - YOLO person detection wrapper using Ultralytics; returns bounding boxes and confidences.
- `tracker.py` - Main detection loop; selects best person target and computes PTZ angles.
- `ptz_control.py` - PTZ aiming logic; converts robot-frame bearings to pan/tilt commands via `PtzClient`.
- `geometry.py` - Coordinate transforms: pixel coordinates -> robot-frame bearing using per-camera yaw and FOV assumptions.
- `io_robot.py` - Robot interface wrapper; handles connection, time sync, and client initialization.
- `config.py` - Configuration management with nested dataclasses; all constants in one place (GPU device, model, thresholds, PTZ params).
- `visualization.py` - OpenCV-based live detection visualization:
  - `draw_detections()`: Annotate single camera frame with bounding boxes and confidence labels
  - `create_grid_layout()`: Arrange 5 cameras in 3x2 grid (640x480 cells, 1920px total width)
  - `show_detections_grid()`: Display interactive window with stats panel and keyboard controls
  - `save_annotated_frames()`: Save debug snapshots to disk
- **`ptz_stream.py`** - **[NEW]** WebRTC real-time video streaming from PTZ camera (see `PTZ_STREAMING_README.md`):
  - `PtzStream`: High-level streaming manager with thread-safe frame queue
  - `PtzStreamConfig`: Stream configuration (bitrate, buffer size, timeout)
  - Independent module for facial recognition / emotion detection integration
- **`ptz_webrtc_client.py`** - **[NEW]** Low-level WebRTC client for Spot CAM:
  - `SpotPtzWebRTCClient`: SDP negotiation and RTCPeerConnection management
  - `SpotPtzVideoTrack`: Video frame queuing from WebRTC stream
- **`test_ptz_stream.py`** - **[NEW]** Standalone PTZ streaming test utility:
  - Command-line interface for testing streams
  - Video recording to MP4
  - Frame display and statistics
- `test_yolo_model.py` - Verify YOLO model loads and show available models/classes.
- `test_yolo_webcam.py` - Benchmark YOLO models on laptop webcam with GPU/CPU performance metrics.

### PTZ WebRTC Streaming Module

For real-time video streaming from the PTZ camera (e.g., for facial recognition downstream), see **[PTZ_STREAMING_README.md](PTZ_STREAMING_README.md)** for complete documentation.

**Quick Start:**
```powershell
# Install WebRTC dependencies
pip install -r requirements_webrtc.txt

# Test PTZ streaming (10 seconds)
python -m people_observer.test_ptz_stream ROBOT_IP --duration 10

# Save stream to video
python -m people_observer.test_ptz_stream ROBOT_IP --duration 30 --save-video ptz.mp4
```

**Features:**
- âœ… Real-time H.264 video stream from PTZ camera
- âœ… Independent of person detection pipeline
- âœ… Thread-safe frame queue for downstream processing
- âœ… Manual start/stop control
- ğŸš§ TODO: Auto-start when person detected (integration hooks in place)

## Requirements
Install the Boston Dynamics Spot SDK wheels first from the sibling `spot-sdk/prebuilt` directory in this workspace (v5.0.1.2), then the Python dependencies in `requirements.txt`.

**Optional**: For PTZ WebRTC streaming, install additional dependencies:
```powershell
pip install -r requirements_webrtc.txt  # aiortc, av for video streaming
```

**Authentication**: Your venv `Activate.ps1` supplies credentials/tokens; **do not include user/password on the CLI**. Scripts that rely on `bosdyn.client.util.authenticate` will use your stored token. Avoid committing secrets.

**Notes**:
- Depends on live robot services: Image, PTZ, Directory (for time sync).
- Camera sources: surround fisheyes (`frontleft_fisheye_image`, `frontright_fisheye_image`, `left_fisheye_image`, `right_fisheye_image`, `back_fisheye_image`).
- PTZ streaming configured via `CompositorClient` and `StreamQualityClient`; adjust settings in `config.py`.
- YOLO model: defaults to `yolov8n.pt` (lightweight); use `yolov8s.pt` or larger for better accuracy.

## Install
1) Activate your venv (with auth in `Activate.ps1`).
2) Install Spot SDK wheels from `../../spot-sdk/prebuilt/*.whl`:
   ```powershell
   pip install ../../spot-sdk/prebuilt/bosdyn_client-5.0.1.2-py3-none-any.whl
   pip install ../../spot-sdk/prebuilt/bosdyn_api-5.0.1.2-py3-none-any.whl
   pip install ../../spot-sdk/prebuilt/bosdyn_core-5.0.1.2-py3-none-any.whl
   pip install ../../spot-sdk/prebuilt/bosdyn_mission-5.0.1.2-py3-none-any.whl
   ```
3) `pip install -r requirements.txt` (includes ultralytics, opencv-python, numpy).

## Testing

### 1. Verify YOLO Model
```powershell
python -m friendly_spot.people_observer.test_yolo_model
```
Shows available YOLOv8 models and verifies the model loads correctly.

### 2. Test Detection (Dry-Run with Visualization)
```powershell
# Single cycle test
python -m friendly_spot.people_observer.app --hostname $env:ROBOT_IP --once --dry-run --visualize

# Continuous test with live visualization
python -m friendly_spot.people_observer.app --hostname $env:ROBOT_IP --dry-run --visualize
```
- `--dry-run`: Skips PTZ commands, only logs what would be sent
- `--visualize`: Shows OpenCV window with all 5 camera views and bounding boxes
- Press 'q' or ESC in visualization window to quit

### 3. Run Live (with PTZ Control)
```powershell
# Normal operation
python -m friendly_spot.people_observer.app --hostname <ROBOT_IP>

# With visualization
python -m friendly_spot.people_observer.app --hostname <ROBOT_IP> --visualize
```

## CLI Arguments
- `--hostname <IP>`: Robot IP address (required)
- `--mode <bearing|transform>`: Coordinate mapping mode (default: bearing)
  - `bearing`: Fast HFOV-based pixel->yaw mapping
  - `transform`: Uses camera intrinsics and frame transforms (more accurate)
- `--once`: Run single detection cycle and exit (for testing)
- `--dry-run`: Skip PTZ commands, log only
- `--visualize`: Show live OpenCV window with detections

## Configuration
Edit `config.py` or use environment variables:
- `YOLO_MODEL`: Path to YOLO model file
- `PEOPLE_OBSERVER_LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR)
- `PEOPLE_OBSERVER_CONFIDENCE`: Minimum detection confidence (0.0-1.0)
- `LOOP_HZ`: Detection loop frequency