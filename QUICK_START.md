# Friendly Spot + People Observer - Quick Reference

## ğŸš€ Quick Commands

### Test Mode (One Cycle, Save Images)
```powershell
python friendly_spot_main.py --robot $env:ROBOT_IP --enable-observer --once --visualize --save-images test_output/ --verbose
```

### Full Demo (Live + Save)
```powershell
python friendly_spot_main.py --robot $env:ROBOT_IP --enable-observer --visualize --save-images demo_output/ --verbose
```

### Perception Only (No Observer)
```powershell
python friendly_spot_main.py --robot $env:ROBOT_IP --visualize --no-execute --verbose
```

### Webcam Development
```powershell
python friendly_spot_main.py --webcam --visualize --verbose
```

## âœ¨ What It Does

### With `--enable-observer`:
1. ğŸ” Detects people in 5 surround cameras (YOLOv11-seg)
2. ğŸ“ Measures distance with depth from segmentation masks
3. ğŸ¥ Points PTZ camera at closest person automatically
4. ğŸ§  Runs perception on PTZ frame (pose/face/emotion/gesture)
5. ğŸ’š Computes comfort and executes behavior
6. ğŸ¨ Shows depth-colored masks + perception overlay

### Without `--enable-observer`:
1. ğŸ¥ Uses PTZ camera stream directly (manual pointing)
2. ğŸ§  Runs perception pipeline
3. ğŸ’š Computes comfort and executes behavior
4. ğŸ¨ Shows perception overlay (no depth masks)

## ğŸ›ï¸ Key Options

| Flag | Effect |
|------|--------|
| `--enable-observer` | âœ… Auto-detect people + PTZ control |
| `--once` | ğŸ”„ Run one cycle and exit |
| `--visualize` | ğŸ‘ï¸ Show live OpenCV window |
| `--save-images DIR` | ğŸ’¾ Save annotated frames |
| `--no-execute` | â›” Perception only (no robot commands) |
| `--verbose` | ğŸ“ Debug logging |
| `--rate HZ` | â±ï¸ Loop frequency (default: 5 Hz) |

## ğŸ¨ Visualization

### Live Window Shows:
- **Depth-colored masks**: Blue (close) â†’ Green â†’ Red (far)
- **Bounding boxes**: Green boxes around people
- **Distance labels**: "Person 0.95 | 2.34m"
- **Pose landmarks**: 33 keypoints
- **Info panel**: Pose, face, emotion, gesture labels

### Press `q` or `ESC` to quit

## ğŸ“ Output Files

Format: `YYYYMMDD_HHMMSS_mmm_pipeline_iter####.jpg`

Each saved frame includes all visualizations.

## ğŸ”§ Configuration

### Change YOLO Model
Edit `people_observer/config.py`:
```python
YOLO_MODEL_PATH = "yolov11n-seg.pt"  # n=fastest, x=most accurate
```

### Change Detection Threshold
```python
MIN_CONFIDENCE = 0.5  # 0.3 for more detections, 0.7 for fewer
```

### Change Loop Rates
```powershell
--rate 2.0  # Slower (2 Hz)
--rate 10.0 # Faster (10 Hz)
```

Observer rate is in `ObserverConfig` (2 Hz surround, 5 Hz PTZ).

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| No person detected | Lower `MIN_CONFIDENCE` to 0.3 |
| PTZ doesn't move | Check `--dry-run` not set in observer |
| Slow performance | Use `yolov11n-seg`, lower `--rate` |
| No depth overlay | Normal for PTZ (uses surround depth) |
| Import errors | Activate venv first |

## ğŸ“Š Performance

- **YOLO detection**: 150ms (yolov11x) â†’ 40ms (yolov11n)
- **Perception**: 200ms total @ 5 Hz
- **Observer loop**: 500ms @ 2 Hz (surround monitoring)

## âœ… System Status

- âœ… YOLOv11-seg with mask extraction
- âœ… Mask-based depth extraction
- âœ… PTZ auto-control from detections
- âœ… Full perception pipeline integration
- âœ… Unified visualization
- âœ… Save frames to disk
- âœ… Once mode for testing
- âœ… Observer bridge fully implemented

## ğŸ¯ Recommended Workflow

1. **Test setup**: `--once --visualize --save-images test/`
2. **Check output**: Look at `test/` frames
3. **Full run**: Remove `--once`, let it loop
4. **Tune if needed**: Adjust confidence/model/rate
5. **Production**: Remove `--visualize` for performance

## ğŸ“š Full Documentation

See `INTEGRATED_SYSTEM_GUIDE.md` for complete details.
